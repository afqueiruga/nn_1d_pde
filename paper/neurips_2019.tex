\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2019

% ready for submission
\usepackage{neurips_2019}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2019}

% to compile a camera-ready version, add the [final] option, e.g.:
     %\usepackage[final]{neurips_2019}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2019}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Investigating Network Architectures on Canonical 1D Partial Differential Equations}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Alejandro Francisco Queiruga\thanks{} \\
  Energy Geosciences Division\\
  Lawrence Berkeley National Lab\\
  Berkeley, CA 94609\\
  \texttt{afqueiruga@lbl.gov} \\
}

\begin{document}

\maketitle

\begin{abstract}
  This work performs a thorough empirical analysis of the application
  of neural networks to solving well understood partial differential
  equations. Only one spatial dimension is treated to , where pentiful analytical solutions can be evaluated to machine precision to provide datasets with no artifacts.
  Burger's equation and the Korteweg-de Vries equation are treated,
  which even are challenging to solve numerically due
  to their nonlinearity and exhibition of shock fronts even in one spatial dimension.
\end{abstract}

\section{Introduction}

Hypotheses:
\begin{enumerate}
  \item Adding adverserial training will improve stability
  \item need 2 history for wave equation
  \item AE will be bad
    \item need a unet or FC to do heat eq
\end{enumerate}

Most extremely challanging physical systems are extremely challenging to describe due to unknown underlying physics or intractable complexities with traditional approaches. Analysis of dynamics data through neural networks and deep learning is a hot topic and very promising approach to learn predictive tools, but the properties are not yet well understood. The problem of discovering dynamics can be stated as follows:
\begin{equation}
\textrm{Given data }\, u(x_i,t_k), \,\textrm{find }f\textrm{ such that }\, u^{k+1}=f(u^k)
\end{equation}
This paper seeks to explore the use of neural networks as an $f$ to discover predictive functions on well known equations (for which decent $f$s are already known.)
Basic one dimensional partial differential equations are always useful to benchmark to any new numerical technique due to well understood properties and known analytical solutions, which can provide as much data as needed without artifacts from synthesis or acquisition.
Multiple standard types of architectures are applied to each of these
canonical equations.
The experiments are designed to take a few minutes of compute time training to replicate with datasets that even fit within a modern cache.
Applying intuition from well understood physics-and-math-up approaches will improve future approaches, providing insights that can hopefully be applied to problems without known physical descriptions but similarities to canonical problems.




\begin{equation}
  u^{k+1}_{i} = \mathtt{stencil}\left( u^k_{i-1}, u^k_{i},u^{k}_{i+1}\right)
\end{equation}
This architecture corresponds to a fringe case neural network: a 3-weight 1D convolutional network with no activation function. Below, it is shown that the standard ADAM optimizer will discover this stencil, albeit slowly.
These architectures are called ``PureStencil'' and ``PureLinear''.

The equations in this investigation are summarized in Table \ref{tab:pdes}. (Technically, all but one are actually 2D when including time.)
Poisson's equation, the heat equation, a linear parabolic equation,

The Wave Equation, a linear hyperbolic equation

  Burgers' equation, a conservative nonlinear equation known for
  producing sharp fronts,

The Korteweg-de Vries Equation equation, a conservative nonlinear equation known for
  producing sharp fronts,

\begin{table}
  \caption{\label{tab:pdes}PDEs under investigation}
  \begin{tabular}{lll}
    \hline
    Name & Equation & Properties\\
    \hline\hline
    Poisson Equation & $u_{xx} = f $ & Linear, elliptic, static \\\hline\
    Heat Equation & $u_{t} = k u_{xx} $ & Linear, parabolic \\\hline
    Wave Equation & $u_{tt} = c^2 u_{xx} $ & Linear, hyperbolic\\\hline
    Advection Equation & $u_{t} + a u_x = 0 $ & Linear, hyperbolic\\\hline
    Burgers' Equation & $u_{t} + u_x u = 0 $ & Conservative, nonlinear, known for shock waves \\\hline
    Viscous Burgers' Equation & $u_{t} + u_x u = \nu u_{xx} $ & Conservative, nonlinear, shock waves \\\hline
    Korteweg-de Vries Equation & $\partial_t u + 6 u \partial_x u + \partial_{xxx}u = 0$ & Conservative, Nonlinear \\\hline
  \end{tabular}
\end{table}

Boundary conditions are neglected for this investigation. When
performing recurrent prediction, the values at the edges of the domain are set to
the known analytical solution to ensure that improper treatment of the
boundaries do not contaminate the results, yielding pure Dirrichlet
boundary conditions every. The boundary domain is extended to the
sencil width.

A dataset is created for each equation where 10 trajectories are solved analytically and evaluated on a grid of 41 points in x and 50 snapshots in $t$. The analytical solutions and their implementations can be found in the accompanying source code.

\begin{table}
  \caption{\label{tab:network}Network Architectures Used}
  \begin{tabular}{lcc}
    \toprule
    Name & Description & No. of Parameters \\
    \midrule
    FC & & \\
    Linear Conv & & 3 \\
    Pure-ConvMLP & &  \\
    Autoencoder & & \\
    U-Net & &  \\
    Recurrent & & \\
    \bottomrule
  \end{tabular}
\end{table}

Question: Do we want to learn $u^{k+1}$ or $\Delta u^{k+1}$?
The paper THAT ONE ABOUT RESNET showed a technique to describe residual networks as iterative time stepping methods to improve training. The effect on training is tried by adding a skip connection to all of the models--- $f(x)=\mathbf{net}(x)+x$---so that $\mathbf{net}$ learns the rate. This would further allow changing $\Delta t$ or generalize to other time integration schemes, but this was not explored.

The time frequency of snapshots used for training will affect training.
Recall the Courant–Friedrichs–Lewy (CFL) condition dictating the ratio
of spatial and temporal discretization spacings to ensure stability
for explicit schemes: $\delta t / \delta x < C$ or $\delta t / \delta
x^2<C$ for some positive $C$.

\begin{equation}
L\left(u^k,u^{k+1}\right) = \left| u^{k+1}_i-f(u^k) \right|
\end{equation}

This effects the network architecture:
\begin{equation}
L\left(\left\{u^k,u^{k-1}...u^{k-P}\right\},u^{k+1}\right) = \left| u^{k+1}_i-f(u^k,...u^{k-P}) \right|
\end{equation}

This does not effect the network architecture:
\begin{equation}
L\left(u^k,\left\{u^{k+1},u^{k+2}...\right\}\right) = \\ \left| u^{k+1}-f(u^k) \right| + \lambda_1  \left| u^{k+2}-f \circ f(u^k) \right| + \lambda_2  \left| u^{k+3}-f \circ f \circ f(u^k) \right| + ...
\end{equation}

Adverserial training is also considered, where
A conditional discriminator $D(y|x)$ is optimized which learns, given $x$, determine if $y$ is the datum or the model prediction. 

\begin{equation}
\mathbb{E}_x\left[ 1 - D\left(f(x)|x\right) \right] + \mathbb{E}_y\left[ D\left(y|x\right)\right]
\end{equation}
For these problems, no stochastic effects are included, and the model and evaluation of the discrimanor are perfectly deterministic. Thus, the inclusion of adverserial is essentially {\em learning a loss function}, replacing the mean-squared-error loss with potentially something better.

\section{Implementation Details}

The datasets were created using Sympy and the entire study is implemented in PyTorch. The experiments were designed to run in a few minutes each, and were executed using a GPU instance Google Colaboratory. The source code and datasets can be found at {\em redacted to preserve blind review.}


\section{Conclusion}
\label{sec:conclusion}

We show that a fringe case of the neural network architecture corresponds to a standard finite difference stencil, and converges to the expected coefficients.

\begin{figure}
  \centering
  \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \caption{Sample figure caption.}
\end{figure}


\begin{table}
  \caption{Sample table title}
  \label{sample-table}
  \centering
  \begin{tabular}{lll}
    \toprule
    \multicolumn{2}{c}{Part}                   \\
    \cmidrule(r){1-2}
    Name     & Description     & Size ($\mu$m) \\
    \midrule
    Dendrite & Input terminal  & $\sim$100     \\
    Axon     & Output terminal & $\sim$10      \\
    Soma     & Cell body       & up to $10^6$  \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Final instructions}

Do not change any aspects of the formatting parameters in the style files.  In
particular, do not modify the width or length of the rectangle the text should
fit into, and do not change font sizes (except perhaps in the
\textbf{References} section; see below). Please note that pages should be
numbered.

\section{Preparing PDF files}

Please prepare submission files with paper size ``US Letter,'' and not, for
example, ``A4.''

Fonts were the main cause of problems in the past years. Your PDF file must only
contain Type 1 or Embedded TrueType fonts. Here are a few instructions to
achieve this.

\begin{itemize}

\item You should directly generate PDF files using \verb+pdflatex+.

\item You can check which fonts a PDF files uses.  In Acrobat Reader, select the
  menu Files$>$Document Properties$>$Fonts and select Show All Fonts. You can
  also use the program \verb+pdffonts+ which comes with \verb+xpdf+ and is
  available out-of-the-box on most Linux machines.

\item The IEEE has recommendations for generating PDF files whose fonts are also
  acceptable for NeurIPS. Please see
  \url{http://www.emfield.org/icuwb2010/downloads/IEEE-PDF-SpecV32.pdf}

\item \verb+xfig+ "patterned" shapes are implemented with bitmap fonts.  Use
  "solid" shapes instead.

\item The \verb+\bbold+ package almost always uses bitmap fonts.  You should use
  the equivalent AMS Fonts:
\begin{verbatim}
   \usepackage{amsfonts}
\end{verbatim}
followed by, e.g., \verb+\mathbb{R}+, \verb+\mathbb{N}+, or \verb+\mathbb{C}+
for $\mathbb{R}$, $\mathbb{N}$ or $\mathbb{C}$.  You can also use the following
workaround for reals, natural and complex:
\begin{verbatim}
   \newcommand{\RR}{I\!\!R} %real numbers
   \newcommand{\Nat}{I\!\!N} %natural numbers
   \newcommand{\CC}{I\!\!\!\!C} %complex numbers
\end{verbatim}
Note that \verb+amsfonts+ is automatically loaded by the \verb+amssymb+ package.

\end{itemize}

If your file contains type 3 fonts or non embedded TrueType fonts, we will ask
you to fix it.

\subsection{Margins in \LaTeX{}}

Most of the margin problems come from figures positioned by hand using
\verb+\special+ or other commands. We suggest using the command
\verb+\includegraphics+ from the \verb+graphicx+ package. Always specify the
figure width as a multiple of the line width as in the example below:
\begin{verbatim}
   \usepackage[pdftex]{graphicx} ...
   \includegraphics[width=0.8\linewidth]{myfile.pdf}
\end{verbatim}
See Section 4.4 in the graphics bundle documentation
(\url{http://mirrors.ctan.org/macros/latex/required/graphics/grfguide.pdf})

A number of width problems arise when \LaTeX{} cannot properly hyphenate a
line. Please give LaTeX hyphenation hints using the \verb+\-+ command when
necessary.

% \subsubsection*{Acknowledgments}

% Use unnumbered third level headings for the acknowledgments. All acknowledgments
% go at the end of the paper. Do not include acknowledgments in the anonymized
% submission, only in the final paper.

\section*{References}

References follow the acknowledgments. Use unnumbered first-level heading for
the references. Any choice of citation style is acceptable as long as you are
consistent. It is permissible to reduce the font size to \verb+small+ (9 point)
when listing the references. {\bf Remember that you can use more than eight
  pages as long as the additional pages contain \emph{only} cited references.}
\medskip

\small

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.

\end{document}
